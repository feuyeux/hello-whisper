#!/usr/bin/env python3
"""Hello Whisper æ¼”ç¤ºè„šæœ¬ - æ”¯æŒæ–‡ä»¶è½¬å½•å’Œå®æ—¶å½•éŸ³è½¬å½•"""
import sys
import os
import time
import whisper
import tempfile
import signal
try:
    import sounddevice as sd
    import numpy as np
    import scipy.io.wavfile as wavfile
    AUDIO_AVAILABLE = True
except ImportError:
    AUDIO_AVAILABLE = False


def record_audio():
    """å½•åˆ¶éŸ³é¢‘å¹¶è¿”å›ä¸´æ—¶æ–‡ä»¶è·¯å¾„"""
    if not AUDIO_AVAILABLE:
        print("âŒ ç¼ºå°‘å½•éŸ³ä¾èµ–: pip install sounddevice numpy scipy")
        return None

    print("ğŸ¤ å¼€å§‹å½•éŸ³... (æŒ‰ Ctrl+C åœæ­¢)")

    sample_rate = 16000
    audio_data = []
    recording = [True]  # ä½¿ç”¨åˆ—è¡¨é¿å…é—­åŒ…é—®é¢˜

    def callback(indata, frames, time, status):
        if recording[0]:
            audio_data.append(indata.copy())

    def signal_handler(sig, frame):
        recording[0] = False

    signal.signal(signal.SIGINT, signal_handler)

    try:
        with sd.InputStream(samplerate=sample_rate, channels=1,
                            callback=callback, dtype='float32'):
            while recording[0]:
                time.sleep(0.1)
    except KeyboardInterrupt:
        pass

    print("\nğŸ›‘ å½•éŸ³ç»“æŸ")

    if not audio_data:
        print("âŒ æ²¡æœ‰å½•åˆ°éŸ³é¢‘")
        return None

    # ä¿å­˜éŸ³é¢‘
    audio_array = np.concatenate(audio_data, axis=0)
    temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
    audio_int16 = (audio_array * 32767).astype(np.int16)
    wavfile.write(temp_file.name, sample_rate, audio_int16)

    print(f"ğŸ’¾ éŸ³é¢‘å·²ä¿å­˜")
    return temp_file.name


def transcribe_audio(audio_file, show_language=False):
    """è½¬å½•éŸ³é¢‘æ–‡ä»¶"""
    print("ğŸ¯ æ­£åœ¨è½¬å½•...")
    available_models = whisper.available_models()
    print(f"å¯ç”¨æ¨¡å‹: {available_models}")
    # å¯ç”¨æ¨¡å‹: ['tiny.en', 'tiny', 'base.en', 'base',
    # 'small.en', 'small', 'medium.en', 'medium',
    # 'large-v1', 'large-v2', 'large-v3', 'large',
    # 'large-v3-turbo', 'turbo']
    model = whisper.load_model("turbo")
    result = model.transcribe(audio_file)

    print(f"\næ–‡æœ¬: {result['text']}")

    if show_language:
        # è¯­è¨€æ£€æµ‹
        audio = whisper.load_audio(audio_file)
        audio = whisper.pad_or_trim(audio)
        mel = whisper.log_mel_spectrogram(
            audio, n_mels=model.dims.n_mels).to(model.device)
        _, probs = model.detect_language(mel)

        print("\nğŸŒ è¯­è¨€æ£€æµ‹:")
        sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)
        for lang, prob in sorted_probs[:3]:
            print(f"  {lang}: {prob:.3f} ({prob*100:.1f}%)")


def show_usage():
    """æ˜¾ç¤ºä½¿ç”¨è¯´æ˜"""
    print("Hello Whisper æ¼”ç¤ºè„šæœ¬")
    print("ç”¨æ³•:")
    print("  python demo.py <éŸ³é¢‘æ–‡ä»¶>     # æ–‡ä»¶è½¬å½•")
    print("  python demo.py <éŸ³é¢‘æ–‡ä»¶> -l  # æ–‡ä»¶è½¬å½• + è¯­è¨€æ£€æµ‹")
    print("  python demo.py record        # å®æ—¶å½•éŸ³è½¬å½•")
    print("  python demo.py record -l     # å®æ—¶å½•éŸ³ + è¯­è¨€æ£€æµ‹")


def main():
    if len(sys.argv) < 2:
        show_usage()
        return

    first_arg = sys.argv[1]
    show_language = '-l' in sys.argv or '-a' in sys.argv

    audio_file = None

    if first_arg == "record":
        print("ğŸ™ï¸ å®æ—¶å½•éŸ³æ¨¡å¼")
        audio_file = record_audio()
        if not audio_file:
            return
    else:
        if not os.path.exists(first_arg):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {first_arg}")
            return
        audio_file = first_arg

    try:
        transcribe_audio(audio_file, show_language)
        print("\nâœ… å®Œæˆ!")

        # æ¸…ç†å½•éŸ³ä¸´æ—¶æ–‡ä»¶
        if first_arg == "record" and audio_file and os.path.exists(audio_file):
            os.unlink(audio_file)
            print("ğŸ—‘ï¸ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
